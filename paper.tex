\documentclass{llncs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{fixltx2e}
\usepackage{subcaption}
\captionsetup{compatibility=false}
\usepackage{tikz}
\usetikzlibrary{trees}

\algnewcommand{\IIf}[1]{\State\algorithmicif\ #1\ \algorithmicthen}
\algnewcommand{\EndIIf}{}
\MakeRobust{\Call}
\newcommand{\True}{\texttt{true}}
\newcommand{\False}{\texttt{false}}
\newcommand{\II}{\mathcal{I}}
\newcommand{\textoverline}[1]{$\overline{\mbox{#1}}$}

\pagestyle{headings}

\begin{document}

\title{A SAT-Based Counterexample Guided Method for Unbounded Synthesis}

\author{Alexander Legg\inst{1} 
    \and Nina Narodytska\inst{2}
    \and Leonid Ryzhyk\inst{2}}

\institute{NICTA\thanks{NICTA is funded by the Australian Government as represented by the Department of Broadband,
    Communications and the Digital Economy and the Australian Research Council through the ICT
    Centre of Excellence program.} and UNSW \\
    \email{alexander.legg@nicta.com.au}
    \and Samsung Research America}

\maketitle

\begin{abstract}

    Reactive synthesis techniques based on constructing the winning region of
    the system have been shown to work well in many cases but suffer from state
    explosion in others.  A different approach, proposed recently, applies SAT
    solvers in a counterexample guided framework to solve the synthesis
    problem.  However, this method is limited to synthesising systems that
    execute for a bounded number of steps and is incomplete for synthesis with
    unbounded safety and reachability objectives.  We present an extension of
    this technique to unbounded synthesis.  Our method applies Craig
    interpolation to abstract game trees produced by counterexample-guided
    search in order to construct a monotonic sequence of may-losing regions.
    Experimental results based on SYNTCOMP 2015 competition benchmarks show
    this to be a promising alternative that solves some previously intractable
    instances.

\end{abstract}

\section{Introduction}

Reactive systems are ubiquitous in real-world problems such as circuit design,
industrial automation, or device drivers. Automatic synthesis can provide a
\emph{correct by construction} controller for a reactive system from a
specification.  However, the reactive synthesis problem is 2EXPTIME-complete so
naive algorithms are infeasible on even simple systems.

Reactive synthesis is formalised as a game between the \emph{controller} and
its \emph{environment}. In this work we focus on safety games, in which the
controller must prevent the environment from forcing the game into an error
state.  Much of the complexity of reactive synthesis stems from tracking the
set of states in which a player is winning.

There are several techniques that aim to mitigate this complexity by
representing states symbolically.  Historically the most successful technique
has been to use \emph{Binary Decision Diagrams} (BDDs).  BDDs efficiently
represent a relation on a set of game variables but in the worst case the
representation may be exponential. This means that BDDs are not a
one-size-fits-all solution for all reactive synthesis specifications.

Advances in SAT solving technology has prompted research into its applicability
to synthesis as an alternative to BDDs. One approach is to find sets of states
in CNF \cite{demiurge}. Another approach is to eschew states and focus on
\emph{runs} of the game. Previous work has applied this idea to realizability
of bounded games \cite{narodytska2014} by forming abstract representations of
the game.  In this paper, we extend this idea to unbounded games by
constructing approximate sets of winning states from abstract trees.

\section{Reactive Synthesis}

A \emph{safety game}, $G = \langle X, L_u, L_c, \delta, I, E, \rangle$,
consists of a set of boolean state variables, sets of uncontrollable and
controllable label variables, a transition relationship $\delta : (X, L_u, L_c)
\to X$, an initial state, and an error state. The \emph{controller} and
\emph{environment} players choose controllable and uncontrollable labels
respectively and the game proceeds according to $\delta$. 

An \emph{run} of a game $(x_0, u_0, c_0), (x_1, u_1, c_1) \dots (x_n, u_n,
c_n)$ is a chain of state and label pairs of length $n$ s.t.  $x_{k+1}
\leftarrow \delta(x_k, u_k, c_k)$. A run is winning for the controller if $x_0
= I \land \forall i \in \{1..n\} (x_i \neq E)$. In a bounded game of rank $n$
all runs are restricted to length $n$, whereas unbounded games consider runs of
infinite length. Since we consider only deterministic games a run is equivalent
to a list of assignments to $L_c$ and $L_u$.

A \emph{controller strategy} $\pi^c : (X, L_u) \to L_c$ is a mapping of states
and uncontrollable inputs to controllable labels. A controller strategy is
winning in a bounded game of rank $n$ if all runs $(x_0, u_0, \pi^c(x_0, u_0)),
(x_1, u_1, \pi^c(x_1, u_1)) \dots (x_n, u_n, \pi^c(x_n, u_n))$ are winning.
Bounded \emph{realizability} is the problem of determining the existence of
such a strategy for a bounded game.

An \emph{environment strategy} $\pi^e : X \to L_u$ is a mapping of states to
uncontrollable labels. A bounded run is winning for the environment if $x_0
= I \land \exists i \in \{1..n\} (x_i = E)$ and an environment strategy is
winning for a bounded game if there exists a run $(x_0, \pi^e(x_1), c_1), (x_1,
\pi^e(x_1), c_1) \dots (x_n, \pi^e(x_n), c_n)$ that wins for the environment.
Safety games are zero sum, therefore the existence of a controller strategy
implies the nonexistence of an environment strategy and vice versa.

\subsection{Abstract Game Trees}

A set of runs can be represented symbolically by a tree of valuations to
controllable and uncontrollable label variables where valuations are allowed to
be \emph{unfixed}. An unfixed value symbolically represents all possible
valuations. Formally, we define the tree as a set of lists of label valuations
and a node in the tree may be identified by the list made by following the path
from the root.

The entire set of runs of a bounded game of rank $n$ is
symbolically represented by a tree of depth $2n$ and width 1 populated by
unfixed edges.  Reducing the set of runs in a game forms an abstract game,
which can be represented symbolically by an \emph{abstract game tree}.

A strategy is equivalent to the set of all runs with the player's labels
obeying the strategy mapping $\pi$. Therefore, a strategy can also be
represented by a tree. Relaxing the restriction of the strategy mapping allows
for a \emph{partial strategy} in which multiple labels are now possible for a
single state.

A strategy or partial strategy can also be thought of as an abstract game. A
partial strategy for the controller is a restriction only on the controllable
labels in the game. So if the environment can not win in the abstract game
equivalent to the controller's partial strategy, then all strategies allowable
by that partial strategy must be winning.

An abstract game tree can be checked for the existence of a winning run by a
SAT solver\cite{narodytska2014}.  The tree must be encoded into CNF by making
copies of the transition relation for each game step in tree. The formula must
also check whether the error state has been reached in no branch/in all
branches for the controller and environment respectively. The functions that
produce these formulas are shown in Algorithm \ref{alg:treeFormula}.

\begin{algorithm}
    \caption{Tree formulas for Controller and Environment respectively}
    \label{alg:treeFormula}
    \begin{algorithmic}[1]
        \Function{treeFormula}{gt}
        \If{$\Call{rank}{gt} == 0$}
        \State \Return{ $\lnot \Call{E}{x^{gt}}$ }
        \Else
        \State \Return{ $\lnot \Call{E}{x^{gt}} \land \bigwedge_{n \in \Call{succ}{gt}}(\Call{$\delta$}{n} \land \Call{label}{n} \land \Call{treeFormula}{n})$ }
        \EndIf
        \EndFunction
        \algstore{tf1}
    \end{algorithmic}

    \begin{algorithmic}[1]
        \algrestore{tf1}
        \Function{\textoverline{treeFormula}}{gt}
        \If{$\Call{rank}{gt} == 0$}
        \State \Return{\Call{E}{$x^{gt}$}}
        \Else
        \State \Return{ $\Call{E}{x^{gt}} \lor \bigvee_{n \in \Call{succ}{gt}}(\Call{$\delta$}{n} \land \Call{label}{n} \land \Call{\textoverline{treeFormula}}{n})$ }
        \EndIf
        \EndFunction
    \end{algorithmic}
\end{algorithm}

When we produce a formula using $\textsc{treeFormula}$ the SAT solver is
playing on behalf on the controller. Any unfixed labels in the tree,
controllable or uncontrollable, will be existentially quantified.  This means
that if there exists a way for the game to be solved when both players
cooperate the SAT solver will find it. If no winning run exists in an abstract
game even when the players are cooperating then there is definitely no winning
run when the opponent is playing adversarily.

\subsection{Counterexample Guided Bounded Synthesis}

By checking for the existence of a winning run for the environment in an
abstract game tree constructed from a partial controller strategy we are
checking if the partial strategy is winning. If no spoiling run can be found
then the partial strategy must always win. If a spoiling run is found then we
then we have a counterexample that proves that the controller strategy is not
winning. This forms the basis of a counterexample guided abstraction refinement
framework that operates on candidate strategies.

This technique was proposed by Narodytska et al \cite{narodytska2014}. In
Section \ref{sect:unbounded} we propose an extension to synthesis of unbounded
games.

The bounded synthesis algorithm begins with an empty game as seen in Figure
\ref{fig:emptytree}.  Initially we are playing on behalf of the environment
because it chooses the first move in each step.  The empty game is passed to
the SAT solver, which searches for a candidate environment strategy.  If a
candidate is found (Figure \ref{fig:candidate}) then it is checked for a
spoiling strategy by solving for the controller.  If no spoiling strategy
exists, that means our candidate is a winning strategy and the algorithm
terminates.  Otherwise we find a counterexample (Figure \ref{fig:cex}), which
is used to refine the empty game tree to include the first move from the
controller's spoiling strategy (Figure \ref{fig:refined}.  The algorithm
continues by finding a new candidate for the environment (Figure
\ref{fig:candidate2}), and a new counterexample to refine the game abstraction
once again (Figure \ref{fig:refined2}). The full algorithm is listed in
Algorithm \ref{alg:bounded}.


\tikzset{every node/.style={solid}}
\tikzstyle{fixed}=[solid]
\tikzstyle{unfixed}=[dash pattern = on 2pt off 2pt]
\begin{figure}
    \centering
    \begin{subfigure}{.3\textwidth}
        \centering
        \begin{tikzpicture}[dash pattern = on 2pt off 2pt, level distance = 10mm]
            \node [circle,draw] (root){}
                child {node [circle,draw] {} edge from parent[unfixed]
                    child {node [circle,draw] {} edge from parent[unfixed]}
                };
        \end{tikzpicture}
        \caption{An empty AGT}
        \label{fig:emptytree}
    \end{subfigure}%
    \begin{subfigure}{.35\textwidth}
        \centering
        \begin{tikzpicture}[dash pattern = on 2pt off 2pt, level distance = 10mm]
            \node [circle,draw] (root) {}
                child {node [circle,draw] {}
                    child {node [circle,draw] {} edge from parent[unfixed]}
                    edge from parent [fixed] node [left] {$u_1$}
                };
        \end{tikzpicture}
        \caption{Environment candidate}
        \label{fig:candidate}
    \end{subfigure}%
    \begin{subfigure}{.4\textwidth}
        \centering
        \begin{tikzpicture}[dash pattern = on 2pt off 2pt, level distance = 10mm]
            \node [circle,draw] (root){}
                child {node [circle,draw] {} edge from parent[unfixed]
                    child {node [circle,draw] {} edge from parent[fixed] node [left] {$c_1$}}
                    edge from parent [fixed] node [left] {$u_1$}
                };
        \end{tikzpicture}
        \caption{Counterexample}
        \label{fig:cex}
    \end{subfigure}


    \begin{subfigure}{.3\textwidth}
        \centering
        \begin{tikzpicture}[dash pattern = on 2pt off 2pt, level distance = 10mm]
            \node [circle,draw] (root){}
                child {node [circle,draw] {} edge from parent[unfixed]
                    child {node [circle,draw] {} edge from parent[fixed] node [left] {$c_1$}}
                };
        \end{tikzpicture}
        \caption{Refined AGT}
        \label{fig:refined}
    \end{subfigure}%
    \begin{subfigure}{.35\textwidth}
        \centering
        \begin{tikzpicture}[dash pattern = on 2pt off 2pt, level distance = 10mm]
            \node [circle,draw] (root) {}
                child {node [circle,draw] {}
                    child {node [circle,draw] {} edge from parent[unfixed]}
                    edge from parent [fixed] node [left] {$u_2$}
                };
        \end{tikzpicture}
        \caption{New candidate}
        \label{fig:candidate2}
    \end{subfigure}%
    \begin{subfigure}{.4\textwidth}
        \centering
        \begin{tikzpicture}[dash pattern = on 2pt off 2pt, level distance = 10mm]
            \node [circle,draw] (root){}
                child {node [circle,draw] {} edge from parent[unfixed]
                    child {node [circle,draw] {} edge from parent[fixed] node [left] {$c_1$}}
                    child {node [circle,draw] {} edge from parent[fixed] node [right] {$c_2$}}
                };
        \end{tikzpicture}
        \caption{Refined AGT}
        \label{fig:refined2}
    \end{subfigure}
    \caption{Abstract game trees}
\end{figure}


\begin{algorithm}
    \begin{algorithmic}[1]
        \Function{solveAbstract}{$\langle s, r \rangle , T$}
        \State $cand \gets $ \Call{findCandidate}{$\langle s, r \rangle, T$} \Comment{Look for a candidate}
        \IIf{$r = n - 1$} \Return $cand$ \EndIIf \Comment{Reached the bound}
        \State $T' \gets T$
        \Loop
            \IIf{$cand = \emptyset $} \Return $\emptyset $ \EndIIf \Comment{No candidate: return with no solution}
            \State $\langle cex, l, u \rangle \gets $ \Call{verify}{$\langle s, r \rangle, T, cand$} \Comment{Verify candidate}
            \IIf{$cex = \False$} \Return $cand$ \EndIIf \Comment{No counterexample: return candidate}
            \State $T' \gets $ \Call{gtAppend}{$T', l, u$} \Comment{Refine AGT with counterexample}
            \State $cand \gets $ \Call{solveAbstract}{$\langle s, r \rangle, T'$} \Comment{Solve refined AGT}
        \EndLoop
        \EndFunction
        \algstore{b1}
    \end{algorithmic}

    \begin{algorithmic}
        \algrestore{b1}
        \Function{findCandidate}{$\langle s, r \rangle, T$}
            \State $sol \gets $ \Call{SAT}{$s(X_T) \land$ \Call{treeFormula}{$T$} }
            \IIf{$sol = \texttt{unsat}$} 
                \Return $\emptyset$ \Comment{No candidate exists}
            \EndIIf
            \State \Return $\{ \langle n, c \rangle | n \in $ \Call{gtNodes}{T} $, c = \Call{sol}{n} \}$ \Comment{Fix candidate moves in $T$}
        \EndFunction
        \algstore{b2}
    \end{algorithmic}

    \begin{algorithmic}
        \algrestore{b2}
        \Function{verify}{$\langle s, r\rangle, gt, cand$}
            \For{$l \in leaves(gt)$}
            \State $\langle r', s'\rangle \gets $ \Call{outcome}{$\langle s, r\rangle, gt, l$} \Comment{Get rank and state at leaf}
                \State $u \gets $ \Call{\textoverline{solveAbstract}}{$\langle r', s' \rangle, \emptyset$} \Comment{Solve for the opponent}
                \IIf{$u \neq \emptyset$} \Return $\langle \False, l, u \rangle$ \EndIIf \Comment{Return counterexample}
            \EndFor
            \State \Return $\langle \True, \emptyset, \emptyset \rangle$
        \EndFunction
    \end{algorithmic}

    \caption{Bounded Synthesis}
    \label{alg:bounded}
\end{algorithm}

\section{Unbounded Synthesis}

\label{sect:unbounded}

Bounded synthesis can be used to prove the existence of a winning strategy for
the environment by providing a witness. For the controller, the strongest claim
that can be made is that the strategy is winning as long as the game does not
extend longer than the bound.

In model checking the maximum bound is decided based on the states of the game
\cite{biere1999}. The na\"ive approach is to use size of the state space as the
bound ($2^{|X|}$). With a bound of this size all states may be explored. One
optimisation is to use the diameter of the game, which is the smallest number
$d$ such that for any state $x$ there is a path of length $\leq d$ to all other
reachable states. However, for large games these bounds are intractable.

When performing model checking or synthesis with BDDs \cite{burch1990} the set
of states that are winning for the environment is iteratively constructed by
computing the states from which the environment can force the game into the
previous winning set. Eventually this process reaches a fixed point and the
total set of environment winning states is known.

A similar concept can be applied to the bounded synthesis algorithm to
iteratively increase the bound of the game and terminate when a fixed point is
reached. When a strategy is found to be winning on an abstract game tree, we
record as winning the states from which the opponent could find no
counterexample. To find these states we use interpolation of subformulas of the
game tree.

\subsection{Learning States with Interpolants}

Given two formulas $A$ and $B$ such that $A \land B$ is unsatisfiable, it is
possible to construct and interpolant $\mathcal{I}$ such that $A \to
\mathcal{I}$, $B \land \mathcal{I}$ is unsatisfiable, and $\mathcal{I}$ refers
only to the intersection of variables in $A$ and $B$. An interpolant can be
constructed efficiently from a resolution proof of the unsatisfiability of $A
\land B$ \cite{pudlak1997}.

\begin{figure}
    \centering
    \begin{tikzpicture}[dash pattern = on 2pt off 2pt, level distance = 10mm]
        \node [] (root){}
            child {node [circle,draw,label=left:$x_1$] {}
                child {node [circle,draw] {}
                    child {node [circle,draw,label=left:$x_0^0$] {} edge from parent[unfixed]}
                    edge from parent [fixed] node [left] {$u_1$}
                }
                child {node [circle,draw] {} edge from parent[unfixed]
                    child {node [circle,draw,label=right:$x_0^1$] {} edge from parent[unfixed]}
                    edge from parent [fixed] node [right] {$u_2$}
                }
                edge from parent [unfixed]
            };
    \end{tikzpicture}
%%%    \begin{tikzpicture}[overlay]
%%%        \draw [rotate = -20] (-1.9,0.35) ellipse (0.7 and 1.3);
%%%    \end{tikzpicture}
    \caption{A controller-losing game tree}
    \label{fig:interpolatetree}
\end{figure}

Consider the snippet of a game tree in Figure \ref{fig:interpolatetree}. The
tree is losing for the controller, the node labelled $x_1$ is at rank 1, and
$x_0^0$ and $x_0^1$ are at rank 0. Since the tree is controller-losing we know
that at least one run represented by the tree contains the error state.  As
a result, $\textsc{treeFormula}(gt)$ is unsatisfiable. If we take the step
$x_1$ to $x_0^0$ and cut it from the rest of the tree then
$\textsc{treeFormula}(step) \land \textsc{treeFormula}(parent)$ must still be
unsatisfiable.

We can construct an interpolant with $A = \textsc{treeFormula}(parent)$ and $B
= \textsc{treeFormula}(step)$. The only variables shared between $A$ and $B$
are the state variables at $x_1$. We know that $B \land \mathcal{I}$ is
unsatisfiable, therefore all states in $\mathcal{I}$ must lose to the
uncontrollable label $u_1$. We also know that $A \to \mathcal{I}$, thus
$\mathcal{I}$ contains all states reachable by the parent tree (on runs that
avoid the error state.)

\begin{algorithm}
    \begin{algorithmic}[1]
        \Require $\sigma(X_T) \land $ \Call{treeFormula}{$T$} $\equiv \bot$
        \Require \emph{Must-invariant} holds
        \Ensure \emph{Must-invariant} holds
        \Ensure $\sigma(X_T) \land B^M \equiv \bot$
        \Function{learn}{$\sigma, T$}
            \IIf{$T = \emptyset$}
                \Return
            \EndIIf
            \State $n \gets $ non-leaf node with max rank
            \State $\langle T_1, T_2 \rangle \gets $ \Call{gtSplit}{$T, n$}
            \State $\mathcal{I} \gets $ \Call{interpolate}{$\sigma(X_T) \land $ \Call{treeFormula}{$T_1$}, \Call{treeFormula}{$T_2$}}
            \State $B^M \gets B^M \lor \mathcal{I}$
            \State \Call{learn}{$\sigma, T_1$}
        \EndFunction
        \algstore{learn}
    \end{algorithmic}
    
    \begin{algorithmic}[1]
        \algrestore{learn}
        \Require $\sigma(X_T) \land $ \Call{\textoverline{treeFormula}}{$T$} $\equiv \bot$
        \Require \emph{May-invariant} holds
        \Ensure \emph{May-invariant} holds
        \Ensure $\sigma(X_T) \land B^m[$\Call{rank}{$T$}$] \equiv \bot$
        \Function{\textoverline{learn}}{$\sigma, T$}
            \IIf{$T = \emptyset$}
                \Return
            \EndIIf
            \State $n \gets $ non-leaf node with max rank
            \State $\langle T_1, T_2 \rangle \gets $ \Call{gtSplit}{$T, n$}
            \State $\mathcal{I} \gets $ \Call{interpolate}{$\sigma(X_T) \land $ \Call{\textoverline{treeFormula}}{$T_1$}, \Call{\textoverline{treeFormula}}{$T_2$}}
            \For{$i = 1$ to \Call{rank}{$n$}}
                \State $B^m[i] \gets B^m[i] \land \lnot \mathcal{I}$
            \EndFor
            \State \Call{\textoverline{learn}}{$\sigma, T_1$}
        \EndFunction
    \end{algorithmic}
    \caption{Learning algorithms}
    \label{alg:learn}
\end{algorithm}

\begin{algorithm}
    \caption{Amended tree formulas for Controller and Environment respectively}
    \label{alg:treeFormula}
    \begin{algorithmic}[1]
        \Function{treeFormula}{gt}
        \If{$\Call{rank}{gt} == 0$}
        \State \Return{ $\lnot \Call{$B^M$}{x^{gt}}$ }
        \Else
        \State \Return{ $\lnot \Call{$B^M$}{x^{gt}} \land \bigwedge_{n \in \Call{succ}{gt}}(\Call{$\delta$}{n} \land \Call{label}{n} \land \Call{treeFormula}{n})$ }
        \EndIf
        \EndFunction
        \algstore{tf2}
    \end{algorithmic}

    \begin{algorithmic}
        \algrestore{tf2}
        \Function{\textoverline{treeFormula}}{gt}
        \If{$\Call{rank}{gt} == 0$}
        \State \Return{\Call{E}{$x^{gt}$}}
        \Else
        \State \Return{ $\Call{$B^m[rank(gt)]$}{x^{gt}} \land$ \\
            $(\Call{E}{x^{gt}} \lor \bigvee_{n \in \Call{succ}{gt}}(\Call{$\delta$}{n} \land \Call{label}{n} \land \Call{$\overline{treeFormula}$}{n}))$ }
        \EndIf
        \EndFunction
    \end{algorithmic}
\end{algorithm}

Now we can consider the step $x_1$ to $x_0^1$, and the parent - now without
either $x_1 \to x_0^0$ or $x_1 \to x_0^1$. The formula
$(\textsc{treeFormula}(parent) \land \mathcal{I}) \land
\textsc{treeFormula}(step)$ must be unsatisfiable. $\mathcal{I}$ contains all
states that lose to $u_1$ so any other state reachable at $x_1$ must lose to
$u_2$. Therefore we can compute another interpolant that contains states that
lose to $u_2$.

This is the foundation for a recursive algorithm that consumes an entire tree
by removing a single step on each iteration (Algorithm \ref{alg:learn}). All
learned states from which the controller \emph{must} lose are recorded in a set
of \emph{bad} states $B^M$.  This algorithm can also be performed on
environment-losing trees with the caveat that any state learnt at a node of
rank $n$ is only known to lose at ranks less than or equal to $n$. We negate
the interpolants we learn this way and record them in a mapping of ranks to
sets of states $B^m$ which \emph{may} be bad states for the controller.

\begin{algorithm}
    \begin{algorithmic}[1]
        \Function{solveUnbounded}{$T$}
            \State $B^M \gets E$
            \State $B^m[0] \gets E$
            \For{$k = 1 \dots$}
                \IIf{\Call{SAT}{$I \land B^M$}}
                    \Return \texttt{unrealisable} \Comment{Losing in the initial state}
                \EndIIf
                \IIf{$\exists i \  B^m[i] \equiv B^m[i+1]$}
                    \Return \texttt{realisable} \Comment{Reached fixed point}
                \EndIIf
                \State $B^m[k] \gets \top$
                \State \Call{solveAbstract}{$\langle I, k \rangle, \emptyset$}
            \EndFor
        \EndFunction
        \algstore{u1}
    \end{algorithmic}

    \begin{algorithmic}
        \algrestore{u1}
        \Function{solveAbstract}{$\langle s, r \rangle , T$}
        \State $cand \gets $ \Call{findCandidate}{$\langle s, r \rangle, T$} \Comment{Look for a candidate}
        \IIf{$r = n - 1$} \Return $cand$ \EndIIf \Comment{Reached the bound}
        \State $T' \gets T$
        \Loop
            \IIf{$cand = \emptyset $} \Return $\emptyset $ \EndIIf \Comment{No candidate: return with no solution}
            \State $\langle cex, l, u \rangle \gets $ \Call{verify}{$\langle s, r \rangle, T, cand$} \Comment{Verify candidate}
            \IIf{$cex = \False$} \Return $cand$ \EndIIf \Comment{No counterexample: return candidate}
            \State $T' \gets $ \Call{gtAppend}{$T', l, u$} \Comment{Refine AGT with counterexample}
            \State $cand \gets $ \Call{solveAbstract}{$\langle s, r \rangle, T'$} \Comment{Solve refined AGT}
        \EndLoop
        \EndFunction
        \algstore{u2}
    \end{algorithmic}

    \begin{algorithmic}
        \algrestore{u2}
        \Function{findCandidate}{$\langle s, r \rangle, T$}
            \State $sol \gets $ \Call{SAT}{$s(X_T) \land$ \Call{treeFormula}{$T$} }
            \If{$sol = \texttt{unsat}$} 
                \State $\sigma \gets $ \Call{generalise}{$s$} \Comment{Expand $s$ to a set of states}
                \State \Call{learn}{$\sigma, T$}
                \State \Return $\emptyset$ \Comment{No candidate exists}
            \Else
                \State \Return $\{ \langle n, c \rangle | n \in $ \Call{gtNodes}{T} $, c = \Call{sol}{n} \}$ \Comment{Fix candidate moves in $T$}
            \EndIf
        \EndFunction
        \algstore{u3}
    \end{algorithmic}

    \begin{algorithmic}
        \algrestore{u3}
        \Function{verify}{$\langle s, r\rangle, gt, cand$}
            \For{$l \in leaves(gt)$}
            \State $\langle r', s'\rangle \gets $ \Call{outcome}{$\langle s, r\rangle, gt, l$} \Comment{Get rank and state at leaf}
                \State $u \gets $ \Call{\textoverline{solveAbstract}}{$\langle r', s' \rangle, \emptyset$} \Comment{Solve for the opponent}
                \IIf{$u \neq \emptyset$} \Return $\langle \False, l, u \rangle$ \EndIIf \Comment{Return counterexample}
            \EndFor
            \State \Return $\langle \True, \emptyset, \emptyset \rangle$
        \EndFunction
    \end{algorithmic}

    \caption{Unbounded Synthesis}
    \label{alg:unbounded}
\end{algorithm}

\subsection{Correctness}

The unbounded synthesis algorithm (Algorimthm \ref{alg:unbounded}) effectively
consists of two communicating solvers: the counterexample-guided bounded
reachability solver and the unbounded solver based on incremental induction.
Fortunately, correctness of both solvers can be established largely
independently, which simplifies the proof of correctness of the overall
algorithm.

We define two global invariants of the algorithm.  The \emph{may-invariant}
states that sets $B^m[i]$ grows monotonically with $i$ and that each $B^m[i+1]$
overapproximates the uncontrollable predecessor of $f^m[i]$: $$\forall
i<k.~B^m[i] \subseteq B^m[i+1], Upre(B^m[i]) \subseteq B^m[i+1].$$

The \emph{must-invariant} guarantees that the must-losing set $B^M$ is an
underapproximation of the actual losing set $B$: $$B^M \subseteq B$$.

The sets $B^m$ and $B^M$ are only modified by the inductive solver, implemented
by \textsc{learn} and \textoverline{\textsc{learn}} functions.  Below we prove that these
functions indeed maintain the invariants.

\subsection{Proof of \textsc{learn}}

We prove that postconditions of \textsc{learn} are satisfied assuming that its
preconditions hold.

Line~(4) splits the tree $T$ into $T_1$ and $T_2$, such that $T_2$ has
depth 1.  Consider formulas $f_1=\sigma(X_T) \land \textsc{treeFormula}(T_1)$
and $F_2 = \textsc{treeFormula}(T_2)$.  These formulas only share variables
$X_n$.  Their conjunction $F_1 \land F_2$ is unsatisfiable, as by construction
any solution of $F_1 \land F_2$ also satisfies $\sigma(X_T) \land
\textsc{treeFormula}(T)$, which is unsatisfiable (precondition (b)).  Hence the
interpolation operation is defined for $F_1$ and $F_2$.  

Intuitively, the interpolant computed in line~(5) overapproximates the set of 
states reachable from $\sigma$ by following the tree from the root node to $n$,
and underapproximates the set of states from which the controller loses against 
tree $T_2$.  

Formally, $\II$ has the property $\II \land F_2 \equiv \bot$.  Since $T_2$ is of depth
1, this means that the controller cannot force the
game into $W^m[|rank|(n-1)]$ when playing against counterexample moves in $T_2$.  Hence, 
$\II \cap Cpre(W^m[|rank|(n)]) = \emptyset$.  Furthermore, since the
may-invariant holds (assumption (c)), $\II \cap Cpre(i) = \emptyset$, for
all $i<|rank|(n)$.  Hence, removing $\II$ from all $W^m[i], i\leq |rank|(n)$
in line~(6) preserves the may-invariant, thus satisfying the first post-condition.

Furthermore, the interpolant satisfies $F_1 \rightarrow \II$, i.e., any assignment to 
$X_n$ that satisfies $\sigma(X_T) \land |treeFormula|(T_1)$ also satisfies $\II$.  Hence, removing $\II$ 
from $W_m[|rank|(n)]$ makes $\sigma(X_T) \land |treeFormula|(T_1)$ unsatisfiable, 
and hence all preconditions of the recursive invocation of \textsf{learn} in line~(7) 
are satisfied.  

At the second las recursive call to \textsf{learn}, tree $T_1$ is empty, $n$ is the root node,
$|treeFormula|(T_1) \equiv W^m[|rank|(T)](X^T)$; hence 
$\sigma(X_T) \land |treeFormula|(T_1) \equiv \sigma(X_T) \land W^m[|rank|(T)](X^T) \equiv \bot$.
Thus the second postcondition of \textsf{learn} holds.

\subsection{Proof of Termination}

We must prove that $|checkRank|$ terminates and that upon 
termination its postcondition holds, i.e., state $s$ is added to $W^M$ if 
its rank is no greater than $k$ or is removed from $W^m[k]$ otherwise.  
Termination follows from completeness of couterexample-guided search, which 
terminates after enumerating all possible opponent moves in the worst case.

Assume that $|rank|(s) > k$.  This means that at some point the algorithm
discovers a counterexample tree of rank $k$ that is losing for the controller 
from state $s$.  The algorithm then invokes the $|learn|$ method, which 
eliminates $s$ from $W^m[k]$.  Alternatively, if $|rank|(s) \leq k$, a 
counterexample tree losing for the environment will be discovered and 
subsequent call to $\overline{|learn|}$ adds $s$ to $W^M$.

\section{Optimisations}

\subsection{Generalising the initial state}

\subsection{Choosing sensible opponent moves}

\section{Evaluation}

\section{Related Work}

\section{Conclusion}


\bibliographystyle{splncs03}
\bibliography{paper}

\end{document}
